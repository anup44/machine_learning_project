<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>ML Project - SVMs</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,600;1,700&family=Montserrat:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&family=Raleway:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/main.css" rel="stylesheet">
  <link href="assets/css/prism.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Impact - v1.2.0
  * Template URL: https://bootstrapmade.com/impact-bootstrap-business-website-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <section id="topbar" class="topbar d-flex align-items-center">
    <div class="container d-flex justify-content-center justify-content-md-between">
      <div class="contact-info d-flex align-items-center">
        <i class="bi bi-envelope d-flex align-items-center"><a href="mailto:contact@example.com">Anup.Bhutada@colorado.edu</a></i>
        <i class="bi bi-phone d-flex align-items-center ms-4"><span>+1 720 312 8601</span></i>
      </div>
      <div class="social-links d-none d-md-flex align-items-center">
        <a href="#" class="twitter"><i class="bi bi-twitter"></i></a>
        <a href="#" class="facebook"><i class="bi bi-facebook"></i></a>
        <a href="#" class="instagram"><i class="bi bi-instagram"></i></a>
        <a href="#" class="linkedin"><i class="bi bi-linkedin"></i></i></a>
      </div>
    </div>
  </section><!-- End Top Bar -->

  <header id="header" class="header d-flex align-items-center">

    <div class="container-fluid container-xl d-flex align-items-center justify-content-between">
      <a href="index.html" class="logo d-flex align-items-center">
        <!-- Uncomment the line below if you also wish to use an image logo -->
        <!-- <img src="assets/img/logo.png" alt=""> -->
        <h1>CSCI 5622 Machine Learning<span>.</span></h1>
      </a>
      <nav id="navbar" class="navbar">
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="index.html#about">Introduction</a></li>
          <li><a href="dataprep.html">Data Prep & EDA</a></li>
          <li><a href="#portfolio">Conclusion</a></li>
          <li class="dropdown"><a href="#"><span>Drop Down</span> <i class="bi bi-chevron-down dropdown-indicator"></i></a>
            <ul>
              <li><a href="clustering.html">Clustering</a></li>
              <!-- <li class="dropdown"><a href="#"><span>Deep Drop Down</span> <i class="bi bi-chevron-down dropdown-indicator"></i></a>
                <ul>
                  <li><a href="#">Deep Drop Down 1</a></li>
                  <li><a href="#">Deep Drop Down 2</a></li>
                  <li><a href="#">Deep Drop Down 3</a></li>
                  <li><a href="#">Deep Drop Down 4</a></li>
                  <li><a href="#">Deep Drop Down 5</a></li>
                </ul>
              </li> -->
              <li><a href="arm.html">ARM</a></li>
              <li><a href="naivebayes.html">Naive Bayes</a></li>
              <li><a href="decisiontree.html">Decision Trees</a></li>
              <li><a href="#">SVM</a></li>
              <li><a href="#">Neural Networks</a></li>
            </ul>
          </li>
          <!-- <li><a href="#contact">Contact</a></li> -->
        </ul>
      </nav><!-- .navbar -->

      <i class="mobile-nav-toggle mobile-nav-show bi bi-list"></i>
      <i class="mobile-nav-toggle mobile-nav-hide d-none bi bi-x"></i>

    </div>
  </header><!-- End Header -->
  <!-- End Header -->

  <main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <div class="breadcrumbs">
      <div class="page-header d-flex align-items-center" style="background-image: url('');">
        <div class="container position-relative">
          <div class="row d-flex justify-content-center">
            <div class="col-lg-6 text-center">
              <h2>Support Vector Machines</h2>
              <!-- <p>Odio et unde deleniti. Deserunt numquam exercitationem. Officiis quo odio sint voluptas consequatur ut a odio voluptatem. Sit dolorum debitis veritatis natus dolores. Quasi ratione sint. Sit quaerat ipsum dolorem.</p> -->
            </div>
          </div>
        </div>
      </div>
      <nav>
        <div class="container">
          <ol>
            <li><a href="index.html">Home</a></li>
            <li>Support Vector Machines</li>
          </ol>
        </div>
      </nav>
    </div><!-- End Breadcrumbs -->

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container" data-aos="fade-up">
        <div class="row justify-content-between gy-4 mt-4">
          <div class="col-lg-10">
            <div class="portfolio-description">
              <h2>Overview</h2>


              <p>
                Support Vector Machines (SVMs) are a type of supervised machine learning algorithm that work by finding a hyperplane that can distinctly separate the classes in the dataset. SVMs try to identify the best hyperplane out of all possible hyperplanes tp separate the data. This is achieved by maximising the margin of separation, that is, the perpendicular distance between data points of both classes. The reason behind maximizing this margin is that a new data point can be classified with more confidence if the hyperplane is farthest from points in both the classes. 
              </p>

              <figure class="d-block text-center">
                <div class="row">
                  <img src="assets/img/svm0.webp" class="col-4 figure-img img-fluid rounded-4 mb-4 mx-auto d-block" alt="A generic square placeholder image with rounded corners in a figure.">
                  <img src="assets/img/svm1.webp" class="col-4 figure-img img-fluid rounded-4 mb-4 mx-auto d-block" alt="A generic square placeholder image with rounded corners in a figure.">
                </div>
                <figcaption class="blockquote-footer">
                  Image from <cite title="Source URL">https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47</cite>
                </figcaption>
              </figure>

              <p>
                The points that are close to the hyperplane are called the support vectors. These points influence the position and orientation of the hyperplane that divides the dataset since SVMs try to maximize the perpendicular distance of the hyperplane from these points. Deleting a support vector can change the position and orientation of the hyperplane since these are the points that help in builing the SVM model.
              </p>

              <p>
                In SVMs, we have a vector $W$ and a constant $b$ that define the location and orientation of the hyperplane in $N$ dimensional space. To classify a point, we use a linear function $\widehat{y} = W^TX + b$ to predict the direction to which the point falls with respect to the hyperplane. If the value of $\widehat{y}$ comes out to be greater than $1$, we classify the point as belonging to one class, and if $\widehat{y}$ is less than $-1$, we classify it into the second class. SVMs are therefore linear separators than can classify datapoints into 2 categories using a hyperplane in $N$ dimensions.
              </p>

              <p>
                To determine the $W$ and $b$ that maximize the margin, we maximize the value $\frac{2}{||W||_2}$ (width of the margin) with respect to the following constraints

                $$y_i \times (W^TX_i+b) - 1 \geq 0$$
                
                where $y_i$ is one of $1$ and $-1$ indicating the class and $X_i$ is the feature vector of the $i^{th}$ data point. 
              </p>


              <p>Using langrange multipliers, we can frame the optimization problem as stated below</p>

              <p>
                $$
                \min L(W, b, \alpha) = \frac{1}{2}||W||^2 - \sum_{i = 1}^n \alpha_i \left[y_i(W^TX_i + b) - 1\right]
                $$

                where $\alpha$ is the lagrange multiplier.
              </p>

              <p>
                This optimization is however solved using the dual form of this optimization problem that offers a way to use the <code>kernel trick</code>. In its dual form, this becomes a maximization problem and can be represented using the following expression.
              </p>

              <p>
                $$
                \max_{\alpha} \sum_{i = 0}^n \alpha_i - \frac{1}{2} \sum_{i = 0}^n \sum_{j = 0}^n \alpha_i \alpha_j y_i y_j (X_i^T.X_j)
                $$

                subject to $\alpha \geq 0$ for all $i = 1,2,..,n$ and $\displaystyle \sum_{i = 1}^n \alpha_iy_i = 0$.
              </p>

              <p>
                W is given by $\displaystyle W = \sum_{i = 0}^n \alpha_iy_iX_i$
              </p>

              <p>
                The steps and explanation for how to arrive at the dual require a very involved explanation that can be found in the links below.

                <ol>
                  <li><a href="https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-1-the-separable-case/" target="_blank">https://machinelearningmastery.com/method-of-lagrange-multipliers-the-theory-behind-support-vector-machines-part-1-the-separable-case/</a></li>
                  <li><a href="https://medium.com/@sathvikchiramana/svm-dual-formulation-7535caa84f17" target="_blank">https://medium.com/@sathvikchiramana/svm-dual-formulation-7535caa84f17</a></li>
                </ol>
              </p>

              <p>
                An interesting thing to note here is that in order to optimize this expression, we do not need the individual feature values of points within $X$. Instead, this expression only requires the dot product of any given pair of points. This allows for a very useful and interesting trick that can allow us to use SVMs, which are linear classifiers, to be used for classifying points using non-linear margins.
              </p>

              <p>
                This is where kernels become very useful in SVMs. A kernel in SVM is a function that takes 2 vectors as inputs and returns the dot product of a transformation of the two vectors. Since we do not need to know the vector that are involved in the dot product for creating the objective function for optimization, we do not have to actually compute the transformed vectors. Instead, we can use simpler fuctions that can give us the same value that would be computed after transforming the original data points and then computing their dot products. This saves a lot of computations and allows SVMs to project points into very high dimensional spaces and still solve the optimization quickly.
              </p>

              <p>
                The image below shows how kernels can be useful for separating points that cannot be separated using a linear hyperplane.
              </p>

              <figure class="d-block text-center">
                <img src="assets/img/svm_kernel_trick.png" class="figure-img img-fluid rounded-4 mb-4 mx-auto d-block" alt="A generic square placeholder image with rounded corners in a figure.">
                <figcaption class="blockquote-footer">
                  Image from <cite title="Source URL">https://www.hashpi.com/the-intuition-behind-kernel-methods</cite>
                </figcaption>
              </figure>

              <p>
                A kernel in SVM is therefore some function $K$ that return the dot product of a transformation $\Phi$ of input vectors without actually computing $\Phi(X_i)$.

                $$K(X_i, X_j) = \Phi(X_1)^T. \Phi(X_2)$$
              </p>

              <p>
                There are many kernels that are commonly used in SVMs. Some popular examples include, <code>linear</code>, <code>Radial Basis Function (RBF)</code>, <code>polynomial</code> and <code>sigmoid</code>. The polynomial kernel can be defined by the following equation,
                $$K_{polynomial}(X_i, X_j) = (X_i^T X_j + r)^d$$.

                The RBF kernel looks like this.
                $$K_{rbf}(X_i, X_j) = \exp{\left[-\frac{||X_i - X_j||^2}{2\sigma^2}\right]}$$

                The RBF kernel can be seen as $\exp{\left[\frac{d_{i,j}}{2\sigma^2}\right]}$ where $d_{i,j}$ is the distance between the points and $\sigma$ is the scaling hyperparameter.
              </p>

              <p>
                Below is an example of how the kernel looks for $\sigma = 1$.
              </p>

              <figure class="d-block text-center">
                <img src="assets/img/rbf_graph.webp" class="figure-img img-fluid rounded-4 mb-4 mx-auto d-block" alt="A generic square placeholder image with rounded corners in a figure.">
                <figcaption class="blockquote-footer">
                  Image from <cite title="Source URL">https://towardsdatascience.com/radial-basis-function-rbf-kernel-the-go-to-kernel-acf0d22c798a</cite>
                </figcaption>
              </figure>

              <p>
                Lets see how these kernels work by going through an example. Consider a $X_1 = (x_{1,1}, x_{1,2})$ and $X_2 = (x_{2,1}, x_{2,2})$ in 2 dimensional space. Lets consider a polynomial kernel defined by 
                $$K_{polynomial}(X_1, X_2) = (X_1^T.X_2 + r)^d$$
              </p>

              <p>
                To prove that this function satisfies the conditions for being a kernel in SVM, we need to show that the result is a dot product of transformation of the 2D vectors. Lets try to prove this by setting $r = 1$ and $d = 2$
              </p>

              <p>
                $$K_{polynomial}(X_1, X_2) = (X_1^T.X_2 + r)^d$$
                $$K_{polynomial}(X_1, X_2) = ((x_{1,1}, x_{1,2}).(x_{2,1}, x_{2,2}) + 1)^2$$
                $$K_{polynomial}(X_1, X_2) = (x_{1,1} x_{2,1} + x_{1,2} x_{2,2} + 1)^2$$
                $$K_{polynomial}(X_1, X_2) = (x_{1,1}^2 x_{2,1}^2 + x_{1,2}^2 x_{2,2}^2 + 2 x_{1,1} x_{2,1} x_{1,2} x_{2,2} + 2 x_{1,1} x_{2,1} + 2 x_{1,2} x_{2,2} + 1)$$
              </p>

              <p>
                This can be written as a dot product of the following 2 vectors.

                $$\Phi(X_1) = (x_{1,1}^2, x_{1,2}^2, \sqrt{2} x_{1,1} x_{1,2}, \sqrt{2} x_{1,1}, \sqrt{2} x_{1,2}, 1)$$
                $$\Phi(X_2) = (x_{2,1}^2, x_{2,2}^2, \sqrt{2} x_{2,1} x_{2,2}, \sqrt{2} x_{2,1}, \sqrt{2} x_{2,2}, 1)$$

                But observe the fact that we did not have to compute these transformations to get the resulting dot product.
              </p>

              <p>
                Lets take a numerical example and verify this result. Let $X_1 = (2,3)$ and $X_2 = (4,5)$
                $$K_{polynomial}(X_1, X_2) = [(2,3).(4,5) + 1]^2 = [8 + 15 + 1]^2 = \textbf{576}$$
                $$\Phi(X) = (x_{1}^2, x_{2}^2, \sqrt{2} x_{1} x_{2}, \sqrt{2} x_{1}, \sqrt{2} x_{2}, 1)$$
                $$\Phi(X_1) = (4, 9, 6\sqrt{2}, 2\sqrt{2}, 3\sqrt{2}, 1)$$
                $$\Phi(X_2) = (16, 25, 20\sqrt{2}, 4\sqrt{2}, 5\sqrt{2}, 1)$$
                
                $$\Phi(X_1).\Phi(X_2) = 64 + 225 + 240 + 16 + 30 + 1 = \textbf{576}$$

                Therefore, we see that

                $$K_{polynomial}(X_1, X_2) = (X_1^T.X_2 + 1)^2 = \Phi(X_1).\Phi(X_2)$$
              </p>

              <h2>Data Prep and Code</h2>

              <p>
                The data has already been cleaned and prepared in <a href="dataprep.html">Data Prep</a> and <a href="clustering.html#:~:text=Text%20Data%20(TFIDF)-,Data%20Preparation%20and%20code,-Here%20we%20do">Clustering</a> parts. The data used in this part is the TFIDF vectorized review text with product category as labels. The TFIDF features are standardized for using with SVMs in this part. The dataset is also filtered for removing highly improbable classes and classes with high overlaps. 
              </p>

              <p>  
                SVM is a supervised learning method because to train SVMs we require labelled data points to define the constraints for the objective function while optimization. The data that was prepared in the sections menrioned above already has labels in the form of categories of products. We will use this to train and test the SVM classifier and observe how well it is able to use product reviews to identify product categories.
              </p>

              <p>Shown below is a sample of the dataset used for this part.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm_data_sample.png">

              <p>
                Since supervised learning methods need to work well on unseen data, that is, the data that was not used for training, this dataset needs to be split into training and testing set. The training set will be used for building the decision tree and the testing set will be used to check if the model is able to predict well on unseen data. This way of measuring the performance of model is required to ensure that our model does not overfit on our data and is able to generalize on new data that might be required to predict on. Since collected data is not a perfect representation of population in most cases, machine learning models generally work better on the data that is used for training and have a slightly, or significantly, worse performance when new data is used for predictions. To account for this bias in the model performance, to some extent, we need to ensure that our training and testing set are disjoint so that we can simulate how the model might perform when it is deployed or used for making predictions.
              </p>

              <p>To create training and testing set, the function <code>train_test_split</code> from the sklearn package was used with <code>test_size = 0.2</code>. Below are two images, showing a sample of the data that was split into training and testing sets.</p>

              <figure class="d-block text-center">
                <img src="assets/img/svm_train_sample.png" class="figure-img img-fluid rounded mx-auto d-block" alt="A generic square placeholder image with rounded corners in a figure.">
                <figcaption class="figure-caption">Training Set sample</figcaption>
              </figure>

              <figure class="d-block text-center">
                <img src="assets/img/svm_test_sample.png" class="figure-img img-fluid rounded" alt="A generic square placeholder image with rounded corners in a figure.">
                <figcaption class="figure-caption">Testing Set sample</figcaption>
              </figure>

              <!-- <figcaption class="figure-caption">A caption for the above image.</figcaption>

              <figcaption class="figure-caption">A caption for the above image.</figcaption> -->

              <p>Link to sample data file and code:</p>

              <ul>
                <li>Data file: <a href="https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_vectorized_sample.csv" target="_blank">https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_vectorized_sample.csv</a></li>
                <li>Data file (scaled): <a href="https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_vectorized_norm_sample.csv" target="_blank">https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_norm_vectorized_sample.csv</a></li>
                <li>SVM Code: <a href="https://github.com/anup44/machine_learning_project/blob/main/svm.ipynb" target="_blank">https://github.com/anup44/machine_learning_project/blob/main/decision_tree.ipynb</a></li>
              </ul>

              <h2>Code</h2>

              <pre><code class="language-python line-numbers">import pandas as pd
import numpy as np
import spacy
import matplotlib.pyplot as plt
from matplotlib import rcParams
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt
import graphviz 

%matplotlib inline

import random

meta_df_1000 = pd.read_parquet('meta_df_1000_cleaned.parquet')
reviews_df = pd.read_parquet('reviews_df_clean.parquet')

merged_df = pd.merge(meta_df_1000, reviews_df, on='asin', how='inner')
merged_df['category_1'] = merged_df.category.apply(lambda x: x[1])
merged_df['rating'] = merged_df['rating'].astype(float)
merged_df.head()

merged_df.info()

merged_df = merged_df[merged_df['category_1']\
                    .isin(['Computers & Accessories', 'Camera & Photo', 
                           'Accessories & Supplies', 'Headphones', 'Car & Vehicle Electronics'])]

nlp = spacy.load('en_core_web_sm')
merged_df['tokens'] = merged_df['content'].apply(lambda x: nlp(x.lower())) 
merged_df.tokens

merged_df['tokens'] = merged_df.tokens.apply(lambda x: [w.lemma_ for w in x if not w.is_stop and not w.is_punct and w.lemma_!=' '])

merged_df['tokens'] = merged_df['tokens'].apply(lambda x: ' '.join(x))
merged_df['tokens'].head(10)

vectorizer = TfidfVectorizer(max_df=0.7)
tokens_vectorized = vectorizer.fit_transform(merged_df['tokens'])
tokens_vectorized

tokens_vectorized.shape

from sklearn.preprocessing import scale
norm_tokens = scale(tokens_vectorized, with_mean=False, axis=0)

np.mean(norm_tokens.toarray(), axis = 0), np.std(norm_tokens.toarray(), axis = 0)

X_train, X_test, y_train, y_test = train_test_split(norm_tokens, merged_df['category_1'], test_size=0.2)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

feat_names = list(vectorizer.get_feature_names_out())

clf_svm1 = SVC(C=1.0, kernel='linear', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm1.fit(X_train, y_train)

y_pred = clf_svm1.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm1.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm2 = SVC(C=1.4, kernel='linear', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm2.fit(X_train, y_train)

y_pred = clf_svm2.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm2.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))


clf_svm2 = SVC(C=1.6, kernel='linear', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200,
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm2.fit(X_train, y_train)

y_pred = clf_svm2.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm2.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm2 = SVC(C=0.8, kernel='linear', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm2.fit(X_train, y_train)

y_pred = clf_svm2.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm2.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm3 = SVC(C=1, kernel='poly', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm3.fit(X_train, y_train)

y_pred = clf_svm3.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm3.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm3 = SVC(C=1.2, kernel='poly', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm3.fit(X_train, y_train)

y_pred = clf_svm3.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm3.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm3 = SVC(C=1.4, kernel='poly', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm3.fit(X_train, y_train)

y_pred = clf_svm3.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm3.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm3 = SVC(C=0.8, kernel='poly', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm3.fit(X_train, y_train)

y_pred = clf_svm3.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm3.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm4 = SVC(C=1, kernel='rbf', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm4.fit(X_train, y_train)

y_pred = clf_svm4.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm4.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm4 = SVC(C=1.2, kernel='rbf', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm4.fit(X_train, y_train)

y_pred = clf_svm4.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm4.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm4 = SVC(C=1.5, kernel='rbf', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm4.fit(X_train, y_train)

y_pred = clf_svm4.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm4.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm5 = SVC(C=0.8, kernel='rbf', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm5.fit(X_train, y_train)

y_pred = clf_svm5.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm5.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

X_train, X_test, y_train, y_test = train_test_split(tokens_vectorized, merged_df['category_1'], test_size=0.2)

clf_svm6 = SVC(C=1.2, kernel='linear', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm6.fit(X_train, y_train)

y_pred = clf_svm6.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm6.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm7 = SVC(C=1.2, kernel='rbf', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm7.fit(X_train, y_train)

y_pred = clf_svm7.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm7.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

clf_svm8 = SVC(C=1.2, kernel='poly', degree=3, 
              gamma='scale', coef0=0.0, 
              shrinking=True, probability=False, 
              tol=0.001, cache_size=200, 
              class_weight=None, verbose=False, 
              max_iter=-1, decision_function_shape='ovr', 
              break_ties=False, random_state=None)

clf_svm8.fit(X_train, y_train)

y_pred = clf_svm8.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=clf_svm8.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))</code></pre>

          <h2>Results</h2>

          <p>Various SVMs were trained and their results were observed. Each of the models trained are discussed below.</p>

          <p>
            The first SVM model was built using the following parameters.
          </p>

          <pre><code class="language-python line-numbers">clf_svm1 = SVC(C=1.0, kernel='linear', degree=3, 
            gamma='scale', coef0=0.0, 
            shrinking=True, probability=False, 
            tol=0.001, cache_size=200, 
            class_weight=None, verbose=False, 
            max_iter=-1, decision_function_shape='ovr', 
            break_ties=False, random_state=None)</code></pre>

          <p>The kernel used is set to <code>linear</code> and the parameter <code>C = 1.0</code>. The value of <code>decision_function_shape</code> is set to <code>ovr</code>.</p>

          <p>The confusion matrix for the predicted classes for this model is shown below.</p>

          <img class="img-fluid rounded-4 mb-4" src="assets/img/svm1_conf_mat_img.png">

          <p>The accuraracy and classification report for the model is shown below.</p>

          <pre><code class="language-css">accuracy: 0.5423728813559322
                           precision    recall  f1-score   support

   Accessories & Supplies       0.32      0.60      0.42       194
           Camera & Photo       0.67      0.51      0.58       298
Car & Vehicle Electronics       0.52      0.48      0.50        52
  Computers & Accessories       0.70      0.59      0.64       394
               Headphones       0.63      0.29      0.40        65

                 accuracy                           0.54      1003
                macro avg       0.57      0.49      0.51      1003
             weighted avg       0.60      0.54      0.55      1003
</code></pre>

          <p>
            The results show that we have an accuracy of about <code>54%</code> and the classification report shows that the model has f1 scores in a similar range for all classes.
          </p>

          <p>
            The next SVM is built using the following parameters.
          </p>

          <pre><code class="language-python line-numbers">clf_svm2 = SVC(C=1.4, kernel='linear', degree=3, 
            gamma='scale', coef0=0.0, 
            shrinking=True, probability=False, 
            tol=0.001, cache_size=200, 
            class_weight=None, verbose=False, 
            max_iter=-1, decision_function_shape='ovr', 
            break_ties=False, random_state=None)</code></pre>

          <p>The confusion matrix for the predicted classes for this model is shown below.</p>

          <img class="img-fluid rounded-4 mb-4" src="assets/img/svm2_conf_mat_img.png">

          <p>The accuraracy and classification report for the model is shown below.</p>

          <pre><code class="language-css">accuracy: 0.53938185443669
                           precision    recall  f1-score   support

   Accessories & Supplies       0.32      0.60      0.42       194
           Camera & Photo       0.66      0.50      0.57       298
Car & Vehicle Electronics       0.51      0.46      0.48        52
  Computers & Accessories       0.69      0.59      0.64       394
               Headphones       0.62      0.28      0.38        65

                 accuracy                           0.54      1003
                macro avg       0.56      0.49      0.50      1003
             weighted avg       0.60      0.54      0.55      1003</code></pre>

              <p>The results obtained from this model are very similar to the previous model. The parameters for the next model are shown below.</p>

              <pre><code class="language-python line-numbers">clf_svm2 = SVC(C=1.6, kernel='linear', degree=3, 
                gamma='scale', coef0=0.0, 
                shrinking=True, probability=False, 
                tol=0.001, cache_size=200, 
                class_weight=None, verbose=False, 
                max_iter=-1, decision_function_shape='ovr', 
                break_ties=False, random_state=None)</code></pre>
  
              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm3_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: 0.5383848454636092
                           precision    recall  f1-score   support

   Accessories & Supplies       0.32      0.60      0.42       194
           Camera & Photo       0.67      0.50      0.57       298
Car & Vehicle Electronics       0.51      0.46      0.48        52
  Computers & Accessories       0.69      0.59      0.64       394
               Headphones       0.62      0.28      0.38        65

                 accuracy                           0.54      1003
                macro avg       0.56      0.49      0.50      1003
             weighted avg       0.60      0.54      0.55      1003
</code></pre>

              <p>
                We see that we again observe very similar results. Increasing the regularization is not improving the performance of the model. In the next model, the regularization is decreased to see how this affects the prediction performance.
              </p>

              <pre><code class="language-python line-numbers">clf_svm2 = SVC(C=0.8, kernel='linear', degree=3, 
                gamma='scale', coef0=0.0, 
                shrinking=True, probability=False, 
                tol=0.001, cache_size=200, 
                class_weight=None, verbose=False, 
                max_iter=-1, decision_function_shape='ovr', 
                break_ties=False, random_state=None)</code></pre>

              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm4_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: 0.5443668993020937
                           precision    recall  f1-score   support

   Accessories & Supplies       0.32      0.60      0.42       194
           Camera & Photo       0.66      0.51      0.57       298
Car & Vehicle Electronics       0.52      0.48      0.50        52
  Computers & Accessories       0.70      0.60      0.64       394
               Headphones       0.63      0.29      0.40        65

                 accuracy                           0.54      1003
                macro avg       0.57      0.49      0.51      1003
             weighted avg       0.60      0.54      0.56      1003
</code></pre>

              <p>
                The results show that the performance is still more or less the same. Lets try to change the kernel in the next model. The kernel used in the next model is <code>polynomial</code> and is defined by setting <code>kernel="poly"</code>.
              </p>

              <pre><code class="language-python line-numbers">clf_svm3 = SVC(C=1, kernel='poly', degree=3, 
                gamma='scale', coef0=0.0, 
                shrinking=True, probability=False, 
                tol=0.001, cache_size=200, 
                class_weight=None, verbose=False, 
                max_iter=-1, decision_function_shape='ovr', 
                break_ties=False, random_state=None)</code></pre>
  
              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm5_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: 0.3938185443668993
                           precision    recall  f1-score   support

   Accessories & Supplies       0.00      0.00      0.00       194
           Camera & Photo       1.00      0.00      0.01       298
Car & Vehicle Electronics       0.00      0.00      0.00        52
  Computers & Accessories       0.39      1.00      0.56       394
               Headphones       0.00      0.00      0.00        65

                 accuracy                           0.39      1003
                macro avg       0.28      0.20      0.11      1003
             weighted avg       0.45      0.39      0.22      1003

</code></pre>

              <p>
                The confusion matrix and classification report show a very poor performance for this kernel. Lets try  different values for regularization using the same kernel in the following models to check if the performance improves.
              </p>

              <pre><code class="language-python line-numbers">clf_svm3 = SVC(C=1.2, kernel='poly', degree=3, 
                gamma='scale', coef0=0.0, 
                shrinking=True, probability=False, 
                tol=0.001, cache_size=200, 
                class_weight=None, verbose=False, 
                max_iter=-1, decision_function_shape='ovr', 
                break_ties=False, random_state=None)</code></pre>

              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm6_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: 0.3938185443668993
                           precision    recall  f1-score   support

   Accessories & Supplies       0.00      0.00      0.00       194
           Camera & Photo       1.00      0.00      0.01       298
Car & Vehicle Electronics       0.00      0.00      0.00        52
  Computers & Accessories       0.39      1.00      0.56       394
               Headphones       0.00      0.00      0.00        65

                 accuracy                           0.39      1003
                macro avg       0.28      0.20      0.11      1003
             weighted avg       0.45      0.39      0.22      1003

</code></pre>

              <p>
                The next model increases the regularization further.
              </p>

              <pre><code class="language-python line-numbers">clf_svm3 = SVC(C=1.4, kernel='poly', degree=3, 
                gamma='scale', coef0=0.0, 
                shrinking=True, probability=False, 
                tol=0.001, cache_size=200, 
                class_weight=None, verbose=False, 
                max_iter=-1, decision_function_shape='ovr', 
                break_ties=False, random_state=None)</code></pre>
  
              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm7_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: 0.3938185443668993
                           precision    recall  f1-score   support

   Accessories & Supplies       0.00      0.00      0.00       194
           Camera & Photo       1.00      0.00      0.01       298
Car & Vehicle Electronics       0.00      0.00      0.00        52
  Computers & Accessories       0.39      1.00      0.56       394
               Headphones       0.00      0.00      0.00        65

                 accuracy                           0.39      1003
                macro avg       0.28      0.20      0.11      1003
             weighted avg       0.45      0.39      0.22      1003
</code></pre>

              <p>
                No change in performance is observed. Lets decrease the regularization in the next model.
              </p>

              <pre><code class="language-python line-numbers">clf_svm3 = SVC(C=0.8, kernel='poly', degree=3, 
                gamma='scale', coef0=0.0, 
                shrinking=True, probability=False, 
                tol=0.001, cache_size=200, 
                class_weight=None, verbose=False, 
                max_iter=-1, decision_function_shape='ovr', 
                break_ties=False, random_state=None)</code></pre>

              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm8_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: 0.3938185443668993
                           precision    recall  f1-score   support

   Accessories & Supplies       0.00      0.00      0.00       194
           Camera & Photo       1.00      0.00      0.01       298
Car & Vehicle Electronics       0.00      0.00      0.00        52
  Computers & Accessories       0.39      1.00      0.56       394
               Headphones       0.00      0.00      0.00        65

                 accuracy                           0.39      1003
                macro avg       0.28      0.20      0.11      1003
             weighted avg       0.45      0.39      0.22      1003
</code></pre>

              <p>
                No change in performance is observed. Lets try another kernel. This time the kernel was set to <code>rbf</code>.
              </p>

              <pre><code class="language-python line-numbers">clf_svm4 = SVC(C=1, kernel='rbf', degree=3, 
                gamma='scale', coef0=0.0, 
                shrinking=True, probability=False, 
                tol=0.001, cache_size=200, 
                class_weight=None, verbose=False, 
                max_iter=-1, decision_function_shape='ovr', 
                break_ties=False, random_state=None)</code></pre>

              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm9_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: 0.4307078763708873
                           precision    recall  f1-score   support

   Accessories & Supplies       0.00      0.00      0.00       194
           Camera & Photo       1.00      0.13      0.23       298
Car & Vehicle Electronics       0.00      0.00      0.00        52
  Computers & Accessories       0.41      1.00      0.58       394
               Headphones       0.00      0.00      0.00        65

                 accuracy                           0.43      1003
                macro avg       0.28      0.23      0.16      1003
             weighted avg       0.46      0.43      0.29      1003</code></pre>
  
              <p>
                The observed performance is better than polynomial kernel but still significantly worse than the linear kernel. Again in the following models, we will try different values for regularization.
              </p>

              <pre><code class="language-python line-numbers">clf_svm4 = SVC(C=1.2, kernel='rbf', degree=3, 
                gamma='scale', coef0=0.0, 
                shrinking=True, probability=False, 
                tol=0.001, cache_size=200, 
                class_weight=None, verbose=False, 
                max_iter=-1, decision_function_shape='ovr', 
                break_ties=False, random_state=None)</code></pre>
  
              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm10_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: 0.47058823529411764
                           precision    recall  f1-score   support

   Accessories & Supplies       0.67      0.01      0.02       194
           Camera & Photo       1.00      0.25      0.40       298
Car & Vehicle Electronics       0.00      0.00      0.00        52
  Computers & Accessories       0.43      1.00      0.60       394
               Headphones       1.00      0.03      0.06        65

                 accuracy                           0.47      1003
                macro avg       0.62      0.26      0.22      1003
             weighted avg       0.66      0.47      0.36      1003</code></pre>
  
              <p>
                The performance is still not in the acceptable ranges.
              </p>

              <pre><code class="language-python line-numbers">clf_svm4 = SVC(C=1.5, kernel='rbf', degree=3, 
                gamma='scale', coef0=0.0, 
                shrinking=True, probability=False, 
                tol=0.001, cache_size=200, 
                class_weight=None, verbose=False, 
                max_iter=-1, decision_function_shape='ovr', 
                break_ties=False, random_state=None)</code></pre>

              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm11_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: 0.5034895314057827
                           precision    recall  f1-score   support

   Accessories & Supplies       1.00      0.05      0.09       194
           Camera & Photo       0.93      0.33      0.49       298
Car & Vehicle Electronics       1.00      0.04      0.07        52
  Computers & Accessories       0.44      0.99      0.61       394
               Headphones       0.83      0.08      0.14        65

                 accuracy                           0.50      1003
                macro avg       0.84      0.30      0.28      1003
             weighted avg       0.75      0.50      0.42      1003</code></pre>
  
              <p>
                Although the performance has increased slightly, a majority of points from all classes are still being classified as <code>Computers & Accessories</code>. Lets try a model with smaller regularization.
              </p>

              <pre><code class="language-python line-numbers">clf_svm5 = SVC(C=0.8, kernel='rbf', degree=3, 
                gamma='scale', coef0=0.0, 
                shrinking=True, probability=False, 
                tol=0.001, cache_size=200, 
                class_weight=None, verbose=False, 
                max_iter=-1, decision_function_shape='ovr', 
                break_ties=False, random_state=None)</code></pre>
  
              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm12_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: 0.40279162512462613
                           precision    recall  f1-score   support

   Accessories & Supplies       0.00      0.00      0.00       194
           Camera & Photo       1.00      0.03      0.06       298
Car & Vehicle Electronics       0.00      0.00      0.00        52
  Computers & Accessories       0.40      1.00      0.57       394
               Headphones       0.00      0.00      0.00        65

                 accuracy                           0.40      1003
                macro avg       0.28      0.21      0.13      1003
             weighted avg       0.45      0.40      0.24      1003</code></pre>
    
                <p>
                  We see a decline in performance.
                </p>

                <p>
                  Throughout all the models above, we do not see an acceptable performance in any other kernel apart from the <code>linear</code> kernel. We have used TFIDF vectorized data for the text in our reviews, and scaled them to have unit standard deviation for using in the models. But is this right? Scaling is important in case of models like SVM since features with varying scales may result in one feature being more important than the others if it has a higher spread or standard deviation. But in this case, although the scales are different in all our features, they effectively represent the same measure. Not only that, TFIDF assigns higher values to words that are rare in the dataset. The reason behind using TFIDF scores is that these scarce words might be more important in differetiating documents in the dataset and that is the very reason they have been used instead of features like Count vectors. By scaling these features, we are essentially taking away some of the information that the TFIDF vectors may contain and that might also be relevant for classification. With this thought, a few models were built using unscaled TFIDF values. Lets observe how the performance of these models compare to the ones discussed above.
                </p>

                <p>Using unscaled TFIDF values, the next model used a <code>linear</code> kernel with <code>C = 1.2</code></p>
  
                <pre><code class="language-python line-numbers">clf_svm6 = SVC(C=1.2, kernel='linear', degree=3, 
                  gamma='scale', coef0=0.0, 
                  shrinking=True, probability=False, 
                  tol=0.001, cache_size=200, 
                  class_weight=None, verbose=False, 
                  max_iter=-1, decision_function_shape='ovr', 
                  break_ties=False, random_state=None)</code></pre>
    
                <p>The confusion matrix for the predicted classes for this model is shown below.</p>
  
                <img class="img-fluid rounded-4 mb-4" src="assets/img/svm13_conf_mat_img.png">
  
                <p>The accuraracy and classification report for the model is shown below.</p>
  
                <pre><code class="language-css">accuracy: 0.7337986041874377
                           precision    recall  f1-score   support

   Accessories & Supplies       0.64      0.55      0.59       175
           Camera & Photo       0.83      0.73      0.78       275
Car & Vehicle Electronics       0.88      0.52      0.65        56
  Computers & Accessories       0.69      0.86      0.77       424
               Headphones       0.87      0.63      0.73        73

                 accuracy                           0.73      1003
                macro avg       0.78      0.66      0.70      1003
             weighted avg       0.74      0.73      0.73      1003</code></pre>
    
                <p>
                  This model shows a huge improvement over all of our previous models. The accuracy and f1 scores across all classes have improved significantly. This maybe because the model sees the differences in features (words) that TFIDF considers more importants further apart than the differences in other words that are more prevalent in the corpus.
                </p>

                <p>The performance on other kernels was also observed.</p>

                <pre><code class="language-python line-numbers">clf_svm7 = SVC(C=1.2, kernel='rbf', degree=3, 
                  gamma='scale', coef0=0.0, 
                  shrinking=True, probability=False, 
                  tol=0.001, cache_size=200, 
                  class_weight=None, verbose=False, 
                  max_iter=-1, decision_function_shape='ovr', 
                  break_ties=False, random_state=None)</code></pre>
        
                <p>The confusion matrix for the predicted classes for this model is shown below.</p>
  
                <img class="img-fluid rounded-4 mb-4" src="assets/img/svm14_conf_mat_img.png">
  
                <p>The accuraracy and classification report for the model is shown below.</p>
  
                <pre><code class="language-css">accuracy: 0.7128614157527418
                           precision    recall  f1-score   support

   Accessories & Supplies       0.70      0.46      0.55       175
           Camera & Photo       0.86      0.74      0.80       275
Car & Vehicle Electronics       0.81      0.30      0.44        56
  Computers & Accessories       0.64      0.88      0.74       424
               Headphones       0.87      0.53      0.66        73

                 accuracy                           0.71      1003
                macro avg       0.77      0.58      0.64      1003
             weighted avg       0.74      0.71      0.70      1003</code></pre>
      
              <p>
                We see a slight decline in performance, but this model is still acceptable. Lets also observe how the <code>polynomial</code> kernal performs.
              </p>

              <pre><code class="language-python line-numbers">clf_svm8 = SVC(C=1.2, kernel='poly', degree=3, 
                gamma='scale', coef0=0.0, 
                shrinking=True, probability=False, 
                tol=0.001, cache_size=200, 
                class_weight=None, verbose=False, 
                max_iter=-1, decision_function_shape='ovr', 
                break_ties=False, random_state=None)</code></pre>
      
              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/svm15_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: acc 0.5194416749750748
                           precision    recall  f1-score   support

   Accessories & Supplies       0.45      0.05      0.09       175
           Camera & Photo       0.85      0.38      0.52       275
Car & Vehicle Electronics       0.00      0.00      0.00        56
  Computers & Accessories       0.47      0.95      0.63       424
               Headphones       1.00      0.07      0.13        73

                 accuracy                           0.52      1003
                macro avg       0.55      0.29      0.27      1003
             weighted avg       0.58      0.52      0.44      1003</code></pre>
      
              <p>
                This model again classified a majority of the points into the <code>Computers & Accessories</code> class and has a poor performance.
              </p>

              
              <p>The plot shown below summarizes the accuracies that were achieved in the models discussed above.</p>

              <div>                        
                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                <script src="https://cdn.plot.ly/plotly-2.18.0.min.js"></script>                
                <div id="2db1477b-108b-42a2-9f5e-12244ff239ba" class="plotly-graph-div" style="height:800px; width:1000px;"></div>            
                <script type="text/javascript">                                    
                  window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("2db1477b-108b-42a2-9f5e-12244ff239ba")) {                    Plotly.newPlot(                        "2db1477b-108b-42a2-9f5e-12244ff239ba",                        [{"alignmentgroup":"True","hovertemplate":"Model=%{x}<br>Accuracy=%{marker.color}<extra></extra>","legendgroup":"","marker":{"color":[0.5423728813559322,0.53938185443669,0.5383848454636092,0.5443668993020937,0.3938185443668993,0.3938185443668993,0.3938185443668993,0.3938185443668993,0.4307078763708873,0.47058823529411764,0.5034895314057827,0.40279162512462613,0.7337986041874377,0.7128614157527418,0.5194416749750748],"coloraxis":"coloraxis","pattern":{"shape":""}},"name":"","offsetgroup":"","orientation":"v","showlegend":false,"textposition":"auto","x":["SVM(C=1.0, kernel=\"linear\")","SVM(C=1.4, kernel=\"linear\")","SVM(C=1.6, kernel=\"linear\")","SVM(C=0.8, kernel=\"linear\")","SVM(C=1.0, kernel=\"poly\")","SVM(C=1.2, kernel=\"poly\")","SVM(C=1.4, kernel=\"poly\")","SVM(C=0.8, kernel=\"poly\")","SVM(C=1.0, kernel=\"rbf\")","SVM(C=1.2, kernel=\"rbf\")","SVM(C=1.5, kernel=\"rbf\")","SVM(C=0.8, kernel=\"rbf\")","SVM(C=1.2, kernel=\"linear\") (unscaled)","SVM(C=1.2, kernel=\"rbf\") (unscaled)","SVM(C=1.2, kernel=\"poly\") (unscaled)"],"xaxis":"x","y":[0.5423728813559322,0.53938185443669,0.5383848454636092,0.5443668993020937,0.3938185443668993,0.3938185443668993,0.3938185443668993,0.3938185443668993,0.4307078763708873,0.47058823529411764,0.5034895314057827,0.40279162512462613,0.7337986041874377,0.7128614157527418,0.5194416749750748],"yaxis":"y","type":"bar"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Model"},"tickangle":-80},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Accuracy"}},"coloraxis":{"colorbar":{"title":{"text":"Accuracy"}},"colorscale":[[0.0,"rgb(247,252,253)"],[0.125,"rgb(229,245,249)"],[0.25,"rgb(204,236,230)"],[0.375,"rgb(153,216,201)"],[0.5,"rgb(102,194,164)"],[0.625,"rgb(65,174,118)"],[0.75,"rgb(35,139,69)"],[0.875,"rgb(0,109,44)"],[1.0,"rgb(0,68,27)"]]},"legend":{"tracegroupgap":0},"margin":{"t":60},"barmode":"relative","height":800,"width":1000,"title":{"text":"Accuracy for models with different hyperparameters"}},                        {"responsive": true}                    )                };                            
                </script>        
              </div>

              <p>
                Through the models that were trained and tested, the best performance was observed in case of the SVM model with a <code>linear</code> kernel using unscaled TFIDF features. This model showed an accuracy of about <code>73%</code> and f1 scores above 0.6 for all classes.
              </p>

              <p>
                Among the kernels we used, we observe the best performance in case of <code>linear</code> kernel. The <code>RBF</code> kernel performed close behind the linear kernel in case of unscaled features and significantly worse for scaled data. The <code>polynomial</code> kenrel had the poorest performance among the 3 kernels that were used here.
              </p>


              <h2>Conclusion</h2>

              <p>
                In the model builing process, several models were built using 3 kernels, <code>linear</code>, <code>RBF</code> and <code>polynomial</code>. We used different regularizations for these kernels to observe the performance on testing dataset. We saw an acceptable performance in case of <code>linear</code> kernel. The dataset used for these models was the TFIDF vectorized text from product reviews that was scaled to have unit variance.
              </p>

              <p>
                In the next few models, we made use of unscaled TDIDF features extracted from review text. It was observed that using unscaled data improved the performance of models on testing set significantly. This observation and the use of review data was justified using the following points.
                <ul>
                  <li>Columns produced from TFIDF scores may have different scales but the measure across all columns is the same.</li>
                  <li>TFIDF values have meanings that is changed when their magnitude changes, therefore scaling these scores can result in loss of meaningful information</li>
                  <li>TFIDF scores assign importance to words based on their prevalence in the corpus. Hence scaling the scores takes away this information and treats all words with equal importance.</li>
                </ul>
              </p>

              <p>
                On using unscaled TFIDF scores, it was observed that the <code>linear</code> and <code>RBF</code> kernel performed significantly better, but the <code>polynomial</code> kernel still struggled to make accurate predictions.
              </p>

              <p>
                Support Vector Machines (SVMs) are a powerful and widely used machine learning algorithm for classification tasks. One of the main advantages of SVMs is their ability to handle high-dimensional data, making them useful in applications such as image recognition and natural language processing. SVMs also have a solid theoretical foundation, making them well-suited for complex classification problems. Additionally, SVMs are less prone to overfitting than other algorithms, as they seek to find the optimal decision boundary that maximizes the margin between different classes. Finally, SVMs can handle non-linear decision boundaries through the use of kernel functions, which transform the input space to a higher dimensional space where the problem becomes linearly separable. Overall, SVMs are a versatile and reliable algorithm that can achieve high accuracy in classification tasks across a range of domains.
              </p>

              <p>
                The exercise also showed that SVMs scale poorly. SVMs are computationally intensive and can be slow to train on large datasets. They also require more memory than other algorithms, which can be an issue with limited resources. Moreover, SVMs are highly sensitive to parameters. SVM performance is highly dependent on the choice of kernel function and other hyperparameters. Choosing the wrong parameters can lead to poor performance, and tuning them can be time-consuming.
              </p>

              <p>
                Support Vector Machines (SVMs) are a popular machine learning algorithm for classification and regression tasks. SVMs work by finding the optimal decision boundary between different classes, which helps them to achieve high accuracy in a variety of domains. They can handle both linear and non-linear decision boundaries through the use of kernel functions, which transform the input space to a higher dimensional space where the problem becomes linearly separable. However, SVMs can be computationally intensive and require careful selection of hyperparameters. Despite these limitations, SVMs remain a powerful and widely used algorithm in the machine learning community, and their versatility and reliability make them a valuable tool in many applications.
              </p>

            </div>


              <div>
                
              </div>

              

          <!-- <div class="col-lg-3">
            <div class="portfolio-info">
              <h3>Project information</h3>
              <ul>
                <li><strong>Category</strong> <span>Web design</span></li>
                <li><strong>Client</strong> <span>ASU Company</span></li>
                <li><strong>Project date</strong> <span>01 March, 2020</span></li>
                <li><strong>Project URL</strong> <a href="#">www.example.com</a></li>
                <li><a href="#" class="btn-visit align-self-start">Visit Website</a></li>
              </ul>
            </div>
          </div> -->

        </div>

      </div>
    </section><!-- End Portfolio Details Section -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer" class="footer">

    <div class="container">
      <div class="row gy-4">

        <div class="col-lg-10 col-12 footer-links">
          <h4>Useful Links</h4>
          <ul>
            <li><a href="https://github.com/anup44/machine_learning_project" target="_blank"><strong>All code files:</strong> https://github.com/anup44/machine_learning_project</a></li>
            <li><a href="https://jmcauley.ucsd.edu/data/amazon_v2/metaFiles2/meta_Electronics.json.gz"><strong>Amazon products metadata for Electrinics category:</strong> https://jmcauley.ucsd.edu/data/amazon_v2/metaFiles2/meta_Electronics.json.gz</a></li>
            <li><a href="https://github.com/anup44/machine_learning_project/blob/main/product_reviews_1000.json"><strong>Scraped Product reviews data:</strong> https://github.com/anup44/machine_learning_project/blob/main/product_reviews_1000.json</a></li>
            <li><a href="https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_vectorized_sample.csv"><strong>TFIDF Vectorized sample data:</strong> https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_vectorized_sample.csv</a></li>
            <li><a href="https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_vectorized_norm_sample.csv"><strong>TFIDF Vectorized (standardized) sample data:</strong> https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_vectorized_norm_sample.csv</a></li>
            <li><a href="https://github.com/anup44/machine_learning_project/blob/main/svm.ipynb" target="_blank"><strong>Code for SVM:</strong> https://github.com/anup44/machine_learning_project/blob/main/svm.ipynb</a></li>
          </ul>
        </div>

      </div>
    </div>

  </footer><!-- End Footer -->
  <!-- End Footer -->

  <a href="#" class="scroll-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>
  <script src="assets/js/prism.js"></script>

</body>

</html>