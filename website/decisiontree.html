<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>ML Project - Decision Trees</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,600;1,700&family=Montserrat:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&family=Raleway:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/main.css" rel="stylesheet">
  <link href="assets/css/prism.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Impact - v1.2.0
  * Template URL: https://bootstrapmade.com/impact-bootstrap-business-website-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <section id="topbar" class="topbar d-flex align-items-center">
    <div class="container d-flex justify-content-center justify-content-md-between">
      <div class="contact-info d-flex align-items-center">
        <i class="bi bi-envelope d-flex align-items-center"><a href="mailto:contact@example.com">Anup.Bhutada@colorado.edu</a></i>
        <i class="bi bi-phone d-flex align-items-center ms-4"><span>+1 720 312 8601</span></i>
      </div>
      <div class="social-links d-none d-md-flex align-items-center">
        <a href="#" class="twitter"><i class="bi bi-twitter"></i></a>
        <a href="#" class="facebook"><i class="bi bi-facebook"></i></a>
        <a href="#" class="instagram"><i class="bi bi-instagram"></i></a>
        <a href="#" class="linkedin"><i class="bi bi-linkedin"></i></i></a>
      </div>
    </div>
  </section><!-- End Top Bar -->

  <header id="header" class="header d-flex align-items-center">

    <div class="container-fluid container-xl d-flex align-items-center justify-content-between">
      <a href="index.html" class="logo d-flex align-items-center">
        <!-- Uncomment the line below if you also wish to use an image logo -->
        <!-- <img src="assets/img/logo.png" alt=""> -->
        <h1>CSCI 5622 Machine Learning<span>.</span></h1>
      </a>
      <nav id="navbar" class="navbar">
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="index.html#about">Introduction</a></li>
          <li><a href="dataprep.html">Data Prep & EDA</a></li>
          <li><a href="#portfolio">Conclusion</a></li>
          <li class="dropdown"><a href="#"><span>Drop Down</span> <i class="bi bi-chevron-down dropdown-indicator"></i></a>
            <ul>
              <li><a href="clustering.html">Clustering</a></li>
              <!-- <li class="dropdown"><a href="#"><span>Deep Drop Down</span> <i class="bi bi-chevron-down dropdown-indicator"></i></a>
                <ul>
                  <li><a href="#">Deep Drop Down 1</a></li>
                  <li><a href="#">Deep Drop Down 2</a></li>
                  <li><a href="#">Deep Drop Down 3</a></li>
                  <li><a href="#">Deep Drop Down 4</a></li>
                  <li><a href="#">Deep Drop Down 5</a></li>
                </ul>
              </li> -->
              <li><a href="arm.html">ARM</a></li>
              <li><a href="naivebayes.html">Naive Bayes</a></li>
              <li><a href="#">Decision Trees</a></li>
              <li><a href="svm.html">SVMs</a></li>
              <li><a href="#">Neural Networks</a></li>
            </ul>
          </li>
          <!-- <li><a href="#contact">Contact</a></li> -->
        </ul>
      </nav><!-- .navbar -->

      <i class="mobile-nav-toggle mobile-nav-show bi bi-list"></i>
      <i class="mobile-nav-toggle mobile-nav-hide d-none bi bi-x"></i>

    </div>
  </header><!-- End Header -->
  <!-- End Header -->

  <main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <div class="breadcrumbs">
      <div class="page-header d-flex align-items-center" style="background-image: url('');">
        <div class="container position-relative">
          <div class="row d-flex justify-content-center">
            <div class="col-lg-6 text-center">
              <h2>Decision Trees</h2>
              <!-- <p>Odio et unde deleniti. Deserunt numquam exercitationem. Officiis quo odio sint voluptas consequatur ut a odio voluptatem. Sit dolorum debitis veritatis natus dolores. Quasi ratione sint. Sit quaerat ipsum dolorem.</p> -->
            </div>
          </div>
        </div>
      </div>
      <nav>
        <div class="container">
          <ol>
            <li><a href="index.html">Home</a></li>
            <li>Decision Trees</li>
          </ol>
        </div>
      </nav>
    </div><!-- End Breadcrumbs -->

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container" data-aos="fade-up">
        <div class="row justify-content-between gy-4 mt-4">
          <div class="col-lg-10">
            <div class="portfolio-description">
              <h2>Overview</h2>

              <p>
                Decision trees are a type of supervised machine learning algorithm that work by recursively splitting data into subsets based on conditions on values of input features. They are generally used for classification and regression tasks. Decision trees make their predictions by applying sequential conditional rules on the feature values that are learnt by observing several input feature values and corresponding target values. These rules can be represented in a tree structure where each node of the tree is split into multiple child nodes based on a condition on an input feature. A point therefore travels from the root node and follows a path defined by the conditions in each node to arrive at a leaf node that assigns a predicted class to the point. 
              </p>

              <figure class="d-block text-center">
                <img src="assets/img/decision-tree-classification-algorithm.png" class="figure-img img-fluid rounded-4 mb-4 mx-auto d-block" alt="A generic square placeholder image with rounded corners in a figure.">
                <figcaption class="blockquote-footer">
                  Image from <cite title="Source URL">https://www.devops.ae/decision-tree-classification-algorithm/</cite>
                </figcaption>
              </figure>

              <p>
                There are several measures such as GINI, entripy and information gain that are used to measure the quality of the split at a node in decision trees. GINI impurity measures the degree of probability of misclassification, while entropy measures the degree of disorder in a set of data. Information gain is the reduction in entropy or GINI impurity achieved by splitting the data on a given feature. At every node, during the training phase, the split with the best information gain is chosen to create the child nodes. 
              </p>

              <p>GINI: <br>

                GINI impurity measures the probability of misclassification. If a dataset contains only one class, then the GINI impurity is 0. On the other hand, if the dataset contains an equal number of samples from each class, then the GINI impurity is 0.5. The formula for GINI impurity is:

                $$\text{GINI}(node) = 1 - \sum_{i=1}^c{p_i}$$
                
                Entropy:<br>

                Entropy measures the level of impurity or uncertainty in a dataset. If a dataset contains only one class, then the entropy is 0. On the other hand, if the dataset contains an equal number of samples from each class, then the entropy is 1. The formula for entropy is:
                
                $$\text{Entropy}(node) = -\sum_{i=0}^c{p_i \times log_2(p_i)}$$
                
                where $c$ is the number of classes in the dataset and $p_i$ is the probability of $i^{th}$ class in the node.<br><br>
                
                Information Gain:<br>

                Information Gain is used to determine the reduction in entropy or GINI impurity after splitting the dataset on a particular feature. The feature with the highest information gain is chosen as the splitting criterion. The formula for information gain is:
                
                $$\text{IG}(node, S) = H(S) - \sum_{s \in S}{\frac{N(s)}{N(S)} * H(s)}$$
                
                where $S$ is the set of all child nodes in the split, $N(s)$ is the number of nodes in node $s$ and H is the measure of impurity.
              </p>

              <p>
                Consider an example dataset that has 5 laptops and 5 tablet computers.
              </p>

              <table class="table" cellpadding="5" cellspacing="1">
                <tr><th>Device Type</tdh><th>Screen Size</th></tr>
                <tr><td>Laptop</td><td>15.2</td></tr>
                <tr><td>Laptop</td><td>14.4</td></tr>
                <tr><td>Tablet</td><td>9</td></tr>
                <tr><td>Laptop</td><td>14</td></tr>
                <tr><td>Tablet</td><td>11</td></tr>
                <tr><td>Tablet</td><td>12.4</td></tr>
                <tr><td>Tablet</td><td>10.2</td></tr>
                <tr><td>Laptop</td><td>12.8</td></tr>
                <tr><td>Laptop</td><td>14.8</td></tr>
                <tr><td>Tablet</td><td>13</td></tr>
              </table>
              <p>
                The entropy for this dataset is given by 
                $$\text{Entropy}(D) = - p_{laptop} \times log_2(p_{laptop}) - p_{tablet} \times log_2(p_{tablet})$$
                $$\text{Entropy}(D) = - \frac{1}{2} \times log_2\left(\frac{1}{2}\right) - \frac{1}{2} \times log_2\left(\frac{1}{2}\right) = 1$$
              </p>
              <p>
                We split this data using the condition screensize $\lt$ 13 to get the following subsets:
              </p>
              <div class="row">
                <div class="col-lg-6">
                  <strong><span>Subset 1</span></strong>
                  <table class="table" cellpadding="5" cellspacing="1">
                    <tr><th>Device Type</tdh><th>Screen Size</th></tr>
                    <tr><td>Tablet</td><td>9</td></tr>
                    <tr><td>Tablet</td><td>11</td></tr>
                    <tr><td>Tablet</td><td>12.4</td></tr>
                    <tr><td>Tablet</td><td>10.2</td></tr>
                    <tr><td>Laptop</td><td>12.8</td></tr>
                  </table>
                </div>
                <div class="col-lg-6">
                  <strong><span>Subset 2</span></strong>
                  <table class="table" cellpadding="5" cellspacing="1">
                    <tr><th>Device Type</tdh><th>Screen Size</th></tr>
                    <tr><td>Laptop</td><td>15.2</td></tr>
                    <tr><td>Laptop</td><td>14.4</td></tr>
                    <tr><td>Laptop</td><td>14</td></tr>
                    <tr><td>Laptop</td><td>14.8</td></tr>
                    <tr><td>Tablet</td><td>13</td></tr>
                  </table>
                </div>
              </div>

              <figure class="d-block text-center">
                <img src="assets/img/dt_example.png" class="figure-img img-fluid rounded-4 mb-4 mx-auto d-block" alt="A generic square placeholder image with rounded corners in a figure.">
                <figcaption class="blockquote-footer">
                  Image by author
                </figcaption>
              </figure>

              <p>
                The extropy of each of these subsets is given by
                $$\text{Entropy}(s_1) = - \frac{4}{5} \times log_2\left(\frac{4}{5}\right) - \frac{1}{5} \times log_2\left(\frac{1}{5}\right) = 0.722$$
                $$\text{Entropy}(s_2) = - \frac{4}{5} \times log_2\left(\frac{4}{5}\right) - \frac{1}{5} \times log_2\left(\frac{1}{5}\right) = 0.722$$
              </p>
              <p>
                The Information Gain for this split will therefore be:
                $$IG = \text{Entropy}(D) - \frac{5}{10} \times \text{Entropy}(s_1) - \frac{5}{10} \times \text{Entropy}(s_2)$$
                $$IG = 1 - 0.5 \times 0.722 - 0.5 \times 0.722 = 0.278$$
              </p>

              <p>The split on the feature screen size gives us an information gain of $0.278$.</p>

              <p>
                When building a decision tree, it potentially possible to create infinitely many trees even if we have one continuous feature, since at every node, we can split the dataset using any value of the feature. We can use this to our advantage by building an esemble model of the multiple decision trees that have trained on the same dataset of different subsets of the same dataset to increase the predictive power and reduce the variance of the model.
              </p>
              <p>A decision tree is a flowchart-like structure where each internal node represents a test on a feature, each branch represents the outcome of the test, and each leaf node represents a decision or class label. DTs are easy to understand, interpret, and visualize, making them a popular choice in data mining, statistics, and other fields.</p>

              <h2>Data Prep and Code</h2>

              <p>
                The data has already been cleaned and prepared in <a href="dataprep.html">Data Prep</a> and <a href="clustering.html#:~:text=Text%20Data%20(TFIDF)-,Data%20Preparation%20and%20code,-Here%20we%20do">Clustering</a> parts. The data used in this part is the TFIDF vectorized review text with product category as labels. The dataset is also filtered for removing highly improbable classes and classes with high overlaps.
              </p>

              <p>  
                Decision trees is a supervised learning method because to train decision trees we require labelled data points to be able to compute GINI index, entropy and information gain. The data that was prepared in the sections described above already has labels in the form of categories of products. We will use this to train and test the decision tree classifier to observe how well it is able to use product reviews to identify product categories.
              </p>

              <p>Shown below is a sample of the dataset used for this part.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/dt_data_sample.png">

              <p>Since supervised learning methods need to work well on unseen data, that is, the data that was not used for training, this dataset needs to be split into training and testing set. The training set will be used for building the decision tree and the testing set will be used to check if the model is able to predict well on unseen data. This way of measuring the performance of model is required to ensure that our model does not overfit on our data and is able to generalize on new data that might be required to predict on. Since collected data is not a perfect representation of population in most cases, machine learning models generally work better on the data that is used for training and have a slightly, or significantly, worse performance when new data is used for predictions. To account for this bias in the model performance, to some extent, we need to ensure that our training and testing set are disjoint so that we can simulate how the model might perform when it is deployed or used for making predictions.</p>

              <p>To create training and testing set, the function <code>train_test_split</code> from the sklearn package was used with <code>test_size = 0.2</code>. Below are two images, showing a sample of the data that was split into training and testing sets.</p>

              <figure class="d-block text-center">
                <img src="assets/img/dt_train_sample.png" class="figure-img img-fluid rounded mx-auto d-block" alt="A generic square placeholder image with rounded corners in a figure.">
                <figcaption class="figure-caption">Training Set sample</figcaption>
              </figure>

              <figure class="d-block text-center">
                <img src="assets/img/dt_test_sample.png" class="figure-img img-fluid rounded" alt="A generic square placeholder image with rounded corners in a figure.">
                <figcaption class="figure-caption">Testing Set sample</figcaption>
              </figure>

              <!-- <figcaption class="figure-caption">A caption for the above image.</figcaption>

              <figcaption class="figure-caption">A caption for the above image.</figcaption> -->

              <p>Link to sample data file and code:</p>

              <ul><li>Data file: <a href="https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_vectorized_sample.csv" target="_blank">https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_vectorized_sample.csv</a></li>
                <li>Decision Tree Code: <a href="https://github.com/anup44/machine_learning_project/blob/main/decision_tree.ipynb" target="_blank">https://github.com/anup44/machine_learning_project/blob/main/decision_tree.ipynb</a></li>
              </ul>

              <h2>Code</h2>

              <pre><code class="language-python line-numbers">import pandas as pd
import numpy as np
import spacy
import matplotlib.pyplot as plt
from matplotlib import rcParams
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt
import graphviz 

%matplotlib inline

import random

meta_df_1000 = pd.read_parquet('meta_df_1000_cleaned.parquet')
reviews_df = pd.read_parquet('reviews_df_clean.parquet')

merged_df = pd.merge(meta_df_1000, reviews_df, on='asin', how='inner')
merged_df['category_1'] = merged_df.category.apply(lambda x: x[1])
merged_df['rating'] = merged_df['rating'].astype(float)
merged_df.head()

merged_df.info()

merged_df = merged_df[merged_df['category_1']\
                    .isin(['Computers & Accessories', 'Camera & Photo', 
                           'Accessories & Supplies', 'Headphones', 'Car & Vehicle Electronics'])]

nlp = spacy.load('en_core_web_sm')
merged_df['tokens'] = merged_df['content'].apply(lambda x: nlp(x.lower())) 
merged_df.tokens

merged_df['tokens'] = merged_df.tokens.apply(lambda x: [w.lemma_ for w in x if not w.is_stop and not w.is_punct and w.lemma_!=' '])

merged_df['tokens'] = merged_df['tokens'].apply(lambda x: ' '.join(x))
merged_df['tokens'].head(10)

vectorizer = TfidfVectorizer(max_df=0.7)
tokens_vectorized = vectorizer.fit_transform(merged_df['tokens'])
tokens_vectorized

tokens_vectorized.shape

X_train, X_test, y_train, y_test = train_test_split(tokens_vectorized, merged_df['category_1'], test_size=0.2)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

feat_names = list(vectorizer.get_feature_names_out())

dt_clf = DecisionTreeClassifier(criterion='entropy', 
                            splitter='best', 
                            max_depth=20, 
                            min_samples_split=8, 
                            min_samples_leaf=4, 
                            min_weight_fraction_leaf=0.0, 
                            max_features=None, 
                            random_state=None, 
                            max_leaf_nodes=None, 
                            min_impurity_decrease=0.0, 
                            class_weight=None,
                            ccp_alpha=0.01)

dt_clf.fit(X_train, y_train)
label_names = [x.replace('&', 'and') for x in dt_clf.classes_]
label_names

tree.plot_tree(dt_clf, filled=True)
dot_data = tree.export_graphviz(dt_clf, out_file=None,
                      feature_names=feat_names,  
                      class_names=label_names,  
                      filled=True, rounded=True,  
                      special_characters=True)                                    
graph = graphviz.Source(dot_data) 
graph.render('dt_text_cat_classification1') 

y_pred = dt_clf.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=dt_clf.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

dt_clf = DecisionTreeClassifier(criterion='entropy',
                            splitter='best',  
                            max_depth=20, 
                            min_samples_split=2, 
                            min_samples_leaf=1, 
                            min_weight_fraction_leaf=0.0, 
                            max_features=None, 
                            random_state=None, 
                            max_leaf_nodes=None, 
                            min_impurity_decrease=0.0, 
                            class_weight=None,
                            ccp_alpha=0.005)

dt_clf.fit(X_train, y_train)
label_names = [x.replace('&', 'and') for x in dt_clf.classes_]
# label_names


tree.plot_tree(dt_clf, filled=True)
dot_data = tree.export_graphviz(dt_clf, out_file=None,
                      feature_names=feat_names,  
                      class_names=label_names,  
                      filled=True, rounded=True,  
                      special_characters=True)                                    
graph = graphviz.Source(dot_data) 
graph.render('dt_text_cat_classification2') 

y_pred = dt_clf.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=dt_clf.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

dt_clf = DecisionTreeClassifier(criterion='entropy',
                            splitter='best', 
                            max_depth=20, 
                            min_samples_split=8, 
                            min_samples_leaf=1, 
                            min_weight_fraction_leaf=0.0, 
                            max_features=None, 
                            random_state=None, 
                            max_leaf_nodes=None, 
                            min_impurity_decrease=0.0, 
                            class_weight=None,
                            ccp_alpha=0.001)

dt_clf.fit(X_train, y_train)
label_names = [x.replace('&', 'and') for x in dt_clf.classes_]
# label_names


tree.plot_tree(dt_clf, filled=True)
dot_data = tree.export_graphviz(dt_clf, out_file=None,
                      feature_names=feat_names,  
                      class_names=label_names,  
                      filled=True, rounded=True,  
                      special_characters=True)                                    
graph = graphviz.Source(dot_data) 
graph.render('dt_text_cat_classification3') 

y_pred = dt_clf.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=dt_clf.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

dt_clf = DecisionTreeClassifier(criterion='entropy',
                            splitter='best',
                            max_depth=20, 
                            min_samples_split=8, 
                            min_samples_leaf=1, 
                            min_weight_fraction_leaf=0.0, 
                            max_features=None, 
                            random_state=None, 
                            max_leaf_nodes=None, 
                            min_impurity_decrease=0.0, 
                            class_weight='balanced',
                            ccp_alpha=0.001)

dt_clf.fit(X_train, y_train)
label_names = [x.replace('&', 'and') for x in dt_clf.classes_]
# label_names


tree.plot_tree(dt_clf, filled=True)
dot_data = tree.export_graphviz(dt_clf, out_file=None,
                      feature_names=feat_names,  
                      class_names=label_names,  
                      filled=True, rounded=True,  
                      special_characters=True)                                    
graph = graphviz.Source(dot_data) 
graph.render('dt_text_cat_classification4') 

y_pred = dt_clf.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=dt_clf.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))

n_classes = np.unique(y_train).shape[0]
# np.hstack(np.atleast_2d(np.unique(y_train, return_counts=True)))
np.atleast_2d(np.unique(y_train, return_counts=True)).T
# np.unique(y_train).shape[0]
display({l: np.power(y_train.shape[0], 1)/(n_classes * np.power(cnt, 1)) \
                                          for l, cnt in np.atleast_2d(np.unique(y_train, return_counts=True)).T})

display({l: np.power(y_train.shape[0], 0.7)/(n_classes * np.power(cnt, 0.7)) \
                                          for l, cnt in np.atleast_2d(np.unique(y_train, return_counts=True)).T})

display(np.unique(y_train, return_counts=True))

n_classes = np.unique(y_train).shape[0]
dt_clf = DecisionTreeClassifier(criterion='entropy',
                            splitter='best',
                            max_depth=28, 
                            min_samples_split=2, 
                            min_samples_leaf=1, 
                            min_weight_fraction_leaf=0.0, 
                            max_features=None, 
                            random_state=None, 
                            max_leaf_nodes=None, 
                            min_impurity_decrease=0.0, 
                            class_weight={l: np.power(y_train.shape[0], 0.72)/(n_classes * np.power(cnt, 0.72)) \
                                          for l, cnt in np.atleast_2d(np.unique(y_train, return_counts=True)).T},
                            ccp_alpha=0.001)

dt_clf.fit(X_train, y_train)
label_names = [x.replace('&', 'and') for x in dt_clf.classes_]
# label_names


tree.plot_tree(dt_clf, filled=True)
dot_data = tree.export_graphviz(dt_clf, out_file=None,
                      feature_names=feat_names,  
                      class_names=label_names,  
                      filled=True, rounded=True,  
                      special_characters=True)                                    
graph = graphviz.Source(dot_data) 
graph.render('dt_text_cat_classification') 

y_pred = dt_clf.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=dt_clf.classes_).plot()
plt.xticks(rotation=90)
plt.show()

print ("acc",accuracy_score(y_test, y_pred))

print (classification_report(y_test, y_pred))</code></pre>

              <h2>Results</h2>

              <p>Various decision trees were trained and their results were observed. Each of the models trained are discussed below.</p>

              <p>
                The first decision tree model was built using the following parameters.
              </p>

              <pre><code class="language-python line-numbers">dt_clf = DecisionTreeClassifier(criterion='entropy',
                splitter='best',
                max_depth=20, 
                min_samples_split=8, 
                min_samples_leaf=4, 
                min_weight_fraction_leaf=0.0, 
                max_features=None, 
                random_state=None, 
                max_leaf_nodes=None, 
                min_impurity_decrease=0.0, 
                class_weight=None,
                ccp_alpha=0.01)</code></pre>

              <p>The measure used for impurity is set to <code>entropy</code> and the <code>max_depth = 20</code>. The value of <code>min_samples_split</code> is also set to <code>8</code> and <code>min_samples_leaf</code> to <code>4</code>. Moreover, the parameter <code>ccp = 0.01</code>.</p>

              <p>
                The decision tree built by fitting the data is shown below.
              </p>
              <div>
                <a href="assets/img/dt_text_cat_classification1.pdf" class="glightbox btn-watch-video d-flex align-items-center">
                  <img class="img-responsive" src="assets/img/dt1_img.png" alt="">
                </a>
                <p class="text-secondary">Click to open PDF</p>
              </div>


              <p>The confusion matrix for the predicted classes for this model is shown below.</p>

              <img class="img-fluid rounded-4 mb-4" src="assets/img/dt1_conf_mat_img.png">

              <p>The accuraracy and classification report for the model is shown below.</p>

              <pre><code class="language-css">accuracy: 0.5952143569292123
                           precision    recall  f1-score   support

   Accessories & Supplies       0.45      0.03      0.06       168
           Camera & Photo       0.83      0.58      0.68       305
Car & Vehicle Electronics       0.71      0.17      0.27        60
  Computers & Accessories       0.52      0.88      0.65       402
               Headphones       0.60      0.74      0.66        68

                 accuracy                           0.60      1003
                macro avg       0.62      0.48      0.46      1003
             weighted avg       0.62      0.60      0.54      1003</code></pre>

              <p>
                The results show that although we have an accuracy of about <code>60%</code>, the confusion matrix shows that the model predicts a large proportion of the points as falling into the <code>Computers & Accessories</code> class. This is our majority class and therefore we get a value of accuracy as 60% whereas the f1 f1-score for minority classes are poor. 
              </p>

              <p>
                This may perhaps be because we have restriced our model from growing using <code>min_samples_split, min_samples_leaf, ccp_alpha</code>. Lets let our tree grow by reducing these restictions and see if the model is able to separate the minority classes. The next tree is built using the following parameters.
              </p>

              <pre><code class="language-python line-numbers">dt_clf = DecisionTreeClassifier(criterion='entropy',
                splitter='best',
                max_depth=20, 
                min_samples_split=2, 
                min_samples_leaf=1, 
                min_weight_fraction_leaf=0.0, 
                max_features=None, 
                random_state=None, 
                max_leaf_nodes=None, 
                min_impurity_decrease=0.0, 
                class_weight=None,
                ccp_alpha=0.005)</code></pre>

                <p>
                  The deision tree built by fitting the data is shown below.
                </p>

                <div>
                  <a href="assets/img/dt_text_cat_classification2.pdf" class="glightbox btn-watch-video d-flex align-items-center">
                    <img class="img-fluid rounded-4 mb-4" src="assets/img/dt2_img.png">
                  </a>
                  <p class="text-secondary">Click to open PDF</p>
                </div>
  
                <p>The confusion matrix for the predicted classes for this model is shown below.</p>
  
                <img class="img-fluid rounded-4 mb-4" src="assets/img/dt2_conf_mat_img.png">
  
                <p>The accuraracy and classification report for the model is shown below.</p>
  
                <pre><code class="language-css">accuracy: 0.6041874376869392
                           precision    recall  f1-score   support

   Accessories & Supplies       0.44      0.04      0.08       168
           Camera & Photo       0.84      0.60      0.70       305
Car & Vehicle Electronics       0.71      0.17      0.27        60
  Computers & Accessories       0.53      0.89      0.66       402
               Headphones       0.61      0.74      0.67        68

                 accuracy                           0.60      1003
                macro avg       0.63      0.49      0.48      1003
             weighted avg       0.62      0.60      0.55      1003</code></pre>

              <p>The results obtained from this model are very similar to the previous model. Maybe the restrictions on the model were not reduced enough. The next model was trained with the following parameters.</p>

              <pre><code class="language-python line-numbers">dt_clf = DecisionTreeClassifier(criterion='entropy',
                splitter='best',
                max_depth=20, 
                min_samples_split=8, 
                min_samples_leaf=1, 
                min_weight_fraction_leaf=0.0, 
                max_features=None, 
                random_state=None, 
                max_leaf_nodes=None, 
                min_impurity_decrease=0.0, 
                class_weight=None,
                ccp_alpha=0.001)</code></pre>

                <p>
                  The deision tree built by fitting the data is shown below.
                </p>

                <div>
                  <a href="assets/img/dt_text_cat_classification3.pdf" class="glightbox btn-watch-video d-flex align-items-center">
                    <img class="img-fluid rounded-4 mb-4" src="assets/img/dt3_img.png">
                  </a>
                  <p class="text-secondary">Click to open PDF</p>
                </div>
  
                <p>The confusion matrix for the predicted classes for this model is shown below.</p>
  
                <img class="img-fluid rounded-4 mb-4" src="assets/img/dt3_conf_mat_img.png">
  
                <p>The accuraracy and classification report for the model is shown below.</p>
  
                <pre><code class="language-css">accuracy: 0.5892323030907278
                           precision    recall  f1-score   support

   Accessories & Supplies       0.37      0.19      0.25       168
           Camera & Photo       0.86      0.57      0.69       305
Car & Vehicle Electronics       0.45      0.22      0.29        60
  Computers & Accessories       0.53      0.84      0.65       402
               Headphones       0.66      0.54      0.60        68

                 accuracy                           0.59      1003
                macro avg       0.58      0.47      0.50      1003
             weighted avg       0.61      0.59      0.57      1003</code></pre>

              <p>
                We see that although the accuracy is more or less the same the f1-score on the first class has improved. In the next model, the performance on the minority classes was attempted to be improved by using <code>class_weight='balanced'</code>. The next model was trained with the following parameters.
              </p>

              <pre><code class="language-python line-numbers">dt_clf = DecisionTreeClassifier(criterion='entropy',
                splitter='best',
                max_depth=20, 
                min_samples_split=8, 
                min_samples_leaf=1, 
                min_weight_fraction_leaf=0.0, 
                max_features=None, 
                random_state=None, 
                max_leaf_nodes=None, 
                min_impurity_decrease=0.0, 
                class_weight='balanced',
                ccp_alpha=0.001)</code></pre>

                <p>
                  The deision tree built by fitting the data is shown below.
                </p>
                
                <div>
                  <a href="assets/img/dt_text_cat_classification4.pdf" class="glightbox btn-watch-video d-flex align-items-center">
                    <img class="img-fluid rounded-4 mb-4" src="assets/img/dt4_img.png">
                  </a>
                  <p class="text-secondary">Click to open PDF</p>
                </div>
               
  
                <p>The confusion matrix for the predicted classes for this model is shown below.</p>
  
                <img class="img-fluid rounded-4 mb-4" src="assets/img/dt4_conf_mat_img.png">
  
                <p>The accuraracy and classification report for the model is shown below.</p>
  
                <pre><code class="language-css">accuracy: 0.45663010967098705
                           precision    recall  f1-score   support

   Accessories & Supplies       0.24      0.85      0.38       168
           Camera & Photo       0.85      0.47      0.60       305
Car & Vehicle Electronics       0.45      0.28      0.35        60
  Computers & Accessories       0.79      0.28      0.42       402
               Headphones       0.66      0.60      0.63        68

                 accuracy                           0.46      1003
                macro avg       0.60      0.50      0.48      1003
             weighted avg       0.68      0.46      0.48      1003</code></pre>

              <p>
                In these results, although the performance on the minority classes has improved, it is seen that a lot points are being classified into the <code>Accessories & Supplies</code> class by our model. This has caused the f1-score on other classes to fall. This is probably because the class weights were increased for the minority classes quite drastically. Although this model is arguably a better model with seemingly matching f1-scores for all classes and can be expected to generalize better on new data points, we want to try to conserve the accuracy and reatain the high f1-scores for majority classes while maximizing the f1-score for minority classes. This is because new data is expected, to some extent, to be from the same distribution as out sample dataset.
              </p>

              <p>
                To achieve this, lets try to reduce the weights of minority classes slightly from what the <code>class_weight='balanced'</code> achieves. The new class weights are generated by using the following expression in python code.
              </p>

              <pre><code class="language-python line-numbers">class_weight = \
      {l: np.power(y_train.shape[0], 0.72)/(n_classes * np.power(cnt, 0.72)) \
      for l, cnt in np.atleast_2d(np.unique(y_train, return_counts=True)).T}</code></pre>

              <p>The original and newly computed class weights are shown below.</p>

              <pre><code class="language-css"># Using class_weight='balanced'
{'Accessories & Supplies': 1.0946793997271487,
 'Camera & Photo': 0.7241877256317689,
 'Car & Vehicle Electronics': 4.0321608040201005,
 'Computers & Accessories': 0.47339233038348083,
 'Headphones': 2.8967509025270757}</code></pre>

                <pre><code class="language-css"># Using new weights
{'Accessories & Supplies': 0.6573699695721993,
 'Camera & Photo': 0.49227163302725147,
 'Car & Vehicle Electronics': 1.6375157627962167,
 'Computers & Accessories': 0.36556436211866106,
 'Headphones': 1.2991126280570986}</code></pre>

              <p>The next model was trained with the following parameters.</p>

              <pre><code class="language-python line-numbers">dt_clf = DecisionTreeClassifier(criterion='entropy',
                splitter='best',
                max_depth=28, 
                min_samples_split=2, 
                min_samples_leaf=1, 
                min_weight_fraction_leaf=0.0, 
                max_features=None, 
                random_state=None, 
                max_leaf_nodes=None, 
                min_impurity_decrease=0.0, 
                class_weight={l: np.power(y_train.shape[0], 0.72)/(n_classes * np.power(cnt, 0.72)) \
                              for l, cnt in np.atleast_2d(np.unique(y_train, return_counts=True)).T},
                ccp_alpha=0.001)</code></pre>

                <p>
                  The deision tree built by fitting the data is shown below.
                </p>

                <div>
                  <a href="assets/img/dt_text_cat_classification5.pdf" class="glightbox btn-watch-video d-flex align-items-center">
                    <img class="img-fluid rounded-4 mb-4" src="assets/img/dt5_img.png">
                  </a>
                  <p class="text-secondary">Click to open PDF</p>
                </div>
  
                <p>The confusion matrix for the predicted classes for this model is shown below.</p>
  
                <img class="img-fluid rounded-4 mb-4" src="assets/img/dt5_conf_mat_img.png">
  
                <p>The accuraracy and classification report for the model is shown below.</p>
  
                <pre><code class="language-css">accuracy: 0.5702891326021934
                           precision    recall  f1-score   support

   Accessories & Supplies       0.35      0.33      0.34       168
           Camera & Photo       0.82      0.54      0.65       305
Car & Vehicle Electronics       0.46      0.30      0.36        60
  Computers & Accessories       0.54      0.72      0.62       402
               Headphones       0.64      0.65      0.64        68

                 accuracy                           0.57      1003
                macro avg       0.56      0.51      0.52      1003
             weighted avg       0.60      0.57      0.57      1003
</code></pre>

              <p>These results have a slightly reduced accuracy from the models without class balancing, but the f1-scores for all minority classes have improved at the cost of slight decreaes in f1-scores of the mojority classes. It can be argued whether this model or the previous is best for prediction.</p>

              <h2>Conclusion</h2>

              <p>
                In the model building process, we tried several hyperparameters to achieve a classification performance that could generalize on new or unseen data. The first model that was trained had too restrictive parameters and prevented the tree from growing sufficiently. We then iterated through the second and third models and found an improved prediction performance on minority classes, specifically <code>Accessories & Supplies</code>. 
              </p>

              <p>
                It was then attempted in model 4 to use class balancing to improve the performance on monority classes. The performance did improve on minority classes, but caused a significant loss of performance on majority classes. To account for this, the class weights were manually defined that retained the performance on majority classes, while trying to improve on minority classes. Model 5 had a matching accuracy to the unbalanced models with improved f1-scores on minority classes.
              </p>

              <p>
                In conclusion, decision trees are a powerful machine learning tool for classifying product categories based on review text data. By using TF-IDF features, we were able to extract meaningful and relevant information from the reviews and train a decision tree classifier that achieved a reasonably high accuracy rate.
              </p>
              <p>
                Our analysis showed that certain keywords or phrases were strongly associated with specific product categories, such as "battery life" and "laptop" for Computers & Accessories, or "ear" and "sound" forHeadphones. These insights can be leveraged by e-commerce businesses to improve their product recommendations and marketing strategies.
              </p>
              <p>  
                However, this exercise also revealed some limitations of decision trees, such as their tendency to overfit the training data and their reliance on explicit feature engineering. Decision trees offer a versatile and interpretable approach to analyzing text data, and their use in product categorization can provide valuable insights for businesses and consumers alike.
              </p>

            </div>


              <div>
                
              </div>

              

          <!-- <div class="col-lg-3">
            <div class="portfolio-info">
              <h3>Project information</h3>
              <ul>
                <li><strong>Category</strong> <span>Web design</span></li>
                <li><strong>Client</strong> <span>ASU Company</span></li>
                <li><strong>Project date</strong> <span>01 March, 2020</span></li>
                <li><strong>Project URL</strong> <a href="#">www.example.com</a></li>
                <li><a href="#" class="btn-visit align-self-start">Visit Website</a></li>
              </ul>
            </div>
          </div> -->

        </div>

      </div>
    </section><!-- End Portfolio Details Section -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer" class="footer">

    <div class="container">
      <div class="row gy-4">

        <div class="col-lg-10 col-12 footer-links">
          <h4>Useful Links</h4>
          <ul>
            <li><a href="https://github.com/anup44/machine_learning_project" target="_blank"><strong>All code files:</strong> https://github.com/anup44/machine_learning_project</a></li>
            <li><a href="https://jmcauley.ucsd.edu/data/amazon_v2/metaFiles2/meta_Electronics.json.gz"><strong>Amazon products metadata for Electrinics category:</strong> https://jmcauley.ucsd.edu/data/amazon_v2/metaFiles2/meta_Electronics.json.gz</a></li>
            <li><a href="https://github.com/anup44/machine_learning_project/blob/main/product_reviews_1000.json"><strong>Scraped Product reviews data:</strong> https://github.com/anup44/machine_learning_project/blob/main/product_reviews_1000.json</a></li>
            <li><a href="https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_vectorized_sample.csv"><strong>TFIDF Vectorized sample data:</strong> https://github.com/anup44/machine_learning_project/blob/main/tokens_tfidf_vectorized_sample.csv</a></li>
            <li><a href="https://github.com/anup44/machine_learning_project/blob/main/decision_tree.ipynb" target="_blank"><strong>Code for Decision Tree:</strong> https://github.com/anup44/machine_learning_project/blob/main/decision_tree.ipynb</a></li>
          </ul>
        </div>

      </div>
    </div>

  </footer><!-- End Footer -->
  <!-- End Footer -->

  <a href="#" class="scroll-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>
  <script src="assets/js/prism.js"></script>

</body>

</html>